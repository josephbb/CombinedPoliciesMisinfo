{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lightweight-blair",
   "metadata": {},
   "source": [
    "<h1>Combining Interventions to reduce the spread of misinformation online: Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulation-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install fastparquet\n",
    "!pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bearing-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up environment\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import pickle\n",
    "import src.utils as srcu\n",
    "import src.segmentation as srcseg\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "import numpy as np\n",
    "#Set up parallel processing\n",
    "pandarallel.initialize(nb_workers=8,verbose=True,progress_bar=True)\n",
    "tqdm.pandas()\n",
    "\n",
    "#Make sure things reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Set up directories\n",
    "root = '.'\n",
    "srcu.create_output_directories(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-instrumentation",
   "metadata": {},
   "source": [
    "<h2>Pull data and segment events</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-forth",
   "metadata": {},
   "source": [
    "<h3>Pull</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gather list of incidents \n",
    "import src.database as sdb\n",
    "engine = sdb.get_engine('/home/joebak/venus_cred.txt')\n",
    "incidents = sdb.list_incidents(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "arbitrary-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need something without spaces and / to call each incident. \n",
    "fix_name = lambda name: name.replace(' ','_').replace('/','_')\n",
    "incidents = incidents[incidents['incident']!='Dominion1']\n",
    "\n",
    "#Dominion is high volume, very noisy, has daily patterns, is prolonged\n",
    "#and doesn't conform to our notion of \"events\". \n",
    "incidents['incident_name'] = incidents['incident'].apply(fix_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "timely-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents.to_parquet(root + '/data/incidents.parquet', compression=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-navigation",
   "metadata": {},
   "source": [
    "<h3>Aggregate</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "central-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents = pd.read_parquet(root + '/data/incidents.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abroad-spelling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.database as sdb\n",
    "engine = sdb.get_engine('/home/joebak/venus_cred.txt')\n",
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine)\n",
    "incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-custody",
   "metadata": {},
   "source": [
    "<h3>Repeat Offenders</h3>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "tribal-youth",
   "metadata": {},
   "outputs": [],
   "source": [
    "removed = pickle.load(open(root + '/data/removed.p','rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "independent-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.database as sdb\n",
    "engine = sdb.get_engine()\n",
    "\n",
    "query_ro = '''SELECT user_screen_name,incident, user_followers_count,row_number() over (partition by user_screen_name order by created_at) \n",
    "            FROM (SELECT DISTINCT ON (user_screen_name, incident) user_screen_name, user_followers_count,\n",
    "            incident, created_at, row_number() over (partition by user_screen_name order by created_at)\n",
    "            FROM public.all_ticket_tweets WHERE user_followers_count > 10000 AND incident IS NOT NULL) AS nested'''\n",
    "\n",
    "query_v = '''SELECT user_screen_name,incident, user_followers_count,row_number() over (partition by user_screen_name order by created_at) \n",
    "            FROM (SELECT DISTINCT ON (user_screen_name, incident) user_screen_name, user_followers_count,\n",
    "            incident, created_at, row_number() over (partition by user_screen_name order by created_at)\n",
    "            FROM public.all_ticket_tweets \n",
    "            WHERE incident IS NOT NULL AND user_verified) AS nested'''\n",
    "\n",
    "query_all = \"SELECT * FROM all_ticket_tweets LIMIT 10;\"\n",
    "ro_all =pd.read_sql(query_ro, con=engine)\n",
    "ro_verified =pd.read_sql(query_v, con=engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fossil-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_all.to_csv(root + '/data/ro_all.csv')\n",
    "ro_verified.to_csv(root + '/data/ro_verified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noticed-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_all = pd.read_csv(root+'/data/ro_all.csv')\n",
    "ro_verified=pd.read_csv(root + '/data/ro_verified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rubber-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repeat_offenders_dict(df, incidents, follower_thresh=1,strikes=3):\n",
    "    repeat_offenders = {}\n",
    "    for incident in incidents:\n",
    "        temp = df[df['incident']==incident]\n",
    "        temp = temp[temp['row_number'] > strikes]\n",
    "        temp = temp[temp['user_followers_count'] > follower_thresh]\n",
    "        repeat_offenders[incident] = temp['user_screen_name'].unique()\n",
    "    return repeat_offenders\n",
    "\n",
    "ro_dict_10k = get_repeat_offenders_dict(ro_all, incidents['incident'],follower_thresh=10000)\n",
    "ro_dict_50k = get_repeat_offenders_dict(ro_all, incidents['incident'],follower_thresh=50000)\n",
    "ro_dict_100k = get_repeat_offenders_dict(ro_all,incidents['incident'], follower_thresh=100000)\n",
    "ro_dict_500k = get_repeat_offenders_dict(ro_all, incidents['incident'],follower_thresh=500000)\n",
    "ro_dict_v = get_repeat_offenders_dict(ro_verified, incidents['incident'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sweet-engineer",
   "metadata": {},
   "outputs": [],
   "source": [
    "ro_dict_modest = {}\n",
    "for item in ro_dict_100k.keys():\n",
    "    temp = np.unique(np.hstack([ro_dict_100k[item], \n",
    "              ro_dict_v[item],\n",
    "              removed])).tolist()\n",
    "    ro_dict_modest[item] = temp\n",
    "\n",
    "ro_dict_aggressive = {}\n",
    "for item in ro_dict_100k.keys():\n",
    "    temp = np.unique(np.hstack([ro_dict_50k[item], \n",
    "              ro_dict_v[item],\n",
    "              removed])).tolist()\n",
    "    ro_dict_aggressive[item] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "apparent-style",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import src.database as sdb\n",
    "engine = sdb.get_engine('/home/joebak/venus_cred.txt')\n",
    "\n",
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_10k,floc='/data/timeseries/10K/')\n",
    "row = incidents.iloc[45]\n",
    "agg_save(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "overall-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.database as sdb\n",
    "engine = sdb.get_engine('/home/joebak/venus_cred.txt')\n",
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_10k,floc='/data/timeseries/10K/')\n",
    "_ = incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "three-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_50k,floc='/data/timeseries/50K/',keep=False)\n",
    "_ = incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "literary-russia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0      True\n",
       "1      True\n",
       "2      True\n",
       "3      True\n",
       "4      True\n",
       "       ... \n",
       "148    True\n",
       "149    True\n",
       "150    True\n",
       "151    True\n",
       "152    True\n",
       "Length: 152, dtype: bool"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_100k,floc='/data/timeseries/100K/',keep=False)\n",
    "incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "meaningful-growth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0      True\n",
       "1      True\n",
       "2      True\n",
       "3      True\n",
       "4      True\n",
       "       ... \n",
       "148    True\n",
       "149    True\n",
       "150    True\n",
       "151    True\n",
       "152    True\n",
       "Length: 152, dtype: bool"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_500k,floc='/data/timeseries/500K/',keep=False)\n",
    "_ = incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spare-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_v,floc='/data/timeseries/Verified/',keep=False)\n",
    "_ = incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "extra-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=removed,floc='/data/timeseries/currently/',keep=False)\n",
    "_ = incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bacterial-large",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b54c290d3c408394ef290092301fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2), Label(value='0 / 2'))), HBox(c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be49f7be5e1c41b198049fd0d5ee40d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=2), Label(value='0 / 2'))), HBox(c…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_modest,floc='/data/timeseries/modest/',keep=False)\n",
    "_ = incidents.T.parallel_apply(agg_save)\n",
    "agg_save = lambda row: sdb.aggregate_and_save(row,engine,removed=ro_dict_aggressive,floc='/data/timeseries/aggressive/',keep=False)\n",
    "_ = incidents.T.parallel_apply(agg_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "later-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get totals banned by policy\n",
    "ro_dicts = {'10K':ro_dict_10k, \n",
    "             '50K':ro_dict_50k,\n",
    "              '100K':ro_dict_100k, \n",
    "              '500K':ro_dict_500k, \n",
    "                'Verified':ro_dict_v,\n",
    "               'Modest':ro_dict_modest, \n",
    "               'Aggressive':ro_dict_aggressive}\n",
    "ban_df = pd.DataFrame()\n",
    "for policy in ['10K','50K', '100K','500K', 'Verified','Modest','Aggressive']:\n",
    "    N_banned = np.unique(np.hstack([ro_dicts[policy][item] for item in ro_dicts['10K'].keys()])).size\n",
    "    ban_df =ban_df.append({'Total removed':N_banned, \n",
    "                   'Policy':policy},ignore_index=True)\n",
    "ban_df=ban_df.append({'Total removed':np.unique(removed).size, \n",
    "                'Policy':'Currently'},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "corporate-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "ban_df.to_parquet(root + '/data/ban_df_counts.parquet',compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-conjunction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
